import os
import random
from collections.abc import Generator, Iterable
from pathlib import Path

from atria_logger import get_logger
from atria_types import (
    DatasetLabels,
    DatasetMetadata,
    DatasetSplitType,
    DocumentContent,
    DocumentInstance,
    Image,
    LayoutAnalysisAnnotation,
)

from atria_datasets import DATASETS, DocumentDataset
from atria_datasets.core.dataset._datasets import DatasetConfig

from .utilities import read_pascal_voc, read_words_json

logger = get_logger(__name__)

_CITATION = """\
@software{smock2021tabletransformer,
    author = {Smock, Brandon and Pesala, Rohith},
    month = {06},
    title = {{Table Transformer}},
    url = {https://github.com/microsoft/table-transformer},
    version = {1.0.0},
    year = {2021}
}
"""

_DESCRIPTION = """\
ICDAR-2013: Towards comprehensive table extraction from unstructured documents.
"""

_HOMEPAGE = "https://github.com/microsoft/table-transformer/"

_LICENSE = "https://github.com/microsoft/table-transformer/blob/main/LICENSE"

_URLS = [
    "https://huggingface.co/datasets/bsmock/FinTabNet.c/resolve/main/FinTabNet.c-PDF_Annotations.tar.gz",
    "https://huggingface.co/datasets/bsmock/FinTabNet.c/resolve/main/FinTabNet.c-Structure.tar.gz",
]

_CLASSES = [
    "table",
    "table column",
    "table row",
    "table column header",
    "table projected row header",
    "table spanning cell",
]


class SplitIterator:
    def __init__(self, split: DatasetSplitType, data_dir: str, seed: int = 42):
        self.split = split
        self.data_dir = Path(data_dir)

        base_path = self.data_dir / "FinTabNet.c-Structure" / "FinTabNet.c-Structure"

        if split == DatasetSplitType.test:
            split_path = "test"
        elif split == DatasetSplitType.validation:
            split_path = "val"
        elif split == DatasetSplitType.train:
            split_path = "train"

        self.xmls_dir = base_path / split_path
        self.images_dir = base_path / "images"
        self.words_dir = base_path / "words"

    def __iter__(self) -> Generator[DocumentInstance, None, None]:
        xml_filenames = [
            elem for elem in os.listdir(self.xmls_dir) if elem.endswith(".xml")
        ]
        random.seed(self.seed)
        random.shuffle(xml_filenames)
        for filename in xml_filenames:
            xml_filepath = self.xmls_dir / filename
            image_file_path = self.images_dir / filename.replace(".xml", ".jpg")
            word_file_path = self.words_dir / filename.replace(".xml", "_words.json")
            yield image_file_path, xml_filepath, word_file_path

    def __len__(self) -> int:
        xml_filenames = [
            elem for elem in os.listdir(self.xmls_dir) if elem.endswith(".xml")
        ]
        return len(xml_filenames)


class FinTabNetConfig(DatasetConfig):
    dataset_name: str = "fintabnet"


@DATASETS.register(
    "fintabnet",
    configs={
        "default": FinTabNetConfig(config_name="default"),
        "1k": FinTabNetConfig(
            config_name="1k",
            max_train_samples=1000,
            max_validation_samples=1000,
            max_test_samples=1000,
        ),
    },
)
class FinTabNet(DocumentDataset):
    __config__ = FinTabNetConfig

    def _download_urls(self) -> list[str]:
        return _URLS

    def _metadata(self) -> DatasetMetadata:
        return DatasetMetadata(
            description=_DESCRIPTION,
            homepage=_HOMEPAGE,
            citation=_CITATION,
            license=_LICENSE,
            dataset_labels=DatasetLabels(layout=_CLASSES),
        )

    def _available_splits(self) -> list[DatasetSplitType]:
        return [
            DatasetSplitType.train,
            DatasetSplitType.validation,
            DatasetSplitType.test,
        ]

    def _split_iterator(self, split: DatasetSplitType, data_dir: str) -> Iterable:
        return SplitIterator(split=split, data_dir=data_dir, seed=self.config.seed)

    def _input_transform(self, inputs: tuple[Path, Path, Path]) -> DocumentInstance:
        image_file_path, xml_filepath, word_file_path = inputs
        image = Image(file_path=str(image_file_path)).load()
        annotated_objects = read_pascal_voc(
            str(xml_filepath),
            labels=_CLASSES,
            image_width=image.width,
            image_height=image.height,
        )
        text_elements = read_words_json(
            str(word_file_path), image_width=image.width, image_height=image.height
        )
        return DocumentInstance(
            sample_id=Path(image_file_path).name,
            image=image,
            content=DocumentContent(text_elements=text_elements),
            annotations=[LayoutAnalysisAnnotation(annotated_objects=annotated_objects)],
        )
